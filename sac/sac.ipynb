{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Soft Actor Critic (SAC) implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "from model import ReplayBuffer\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env=None):\n",
    "        super().__init__()\n",
    "        self.input_dim = env.observation_space.shape[0]             \n",
    "        self.output_dim = env.action_space.shape[0]\n",
    "        \n",
    "        self.l1 = nn.Linear(self.input_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        \n",
    "        self.mu = nn.Linear(300, self.output_dim)\n",
    "        self.std = nn.Linear(300, self.output_dim)\n",
    "        \n",
    "        self.apply(weights_init_)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = F.relu(self.l1(x))\n",
    "        a = F.relu(self.l2(a))\n",
    "        \n",
    "        mu = self.mu(a)\n",
    "        log_std = torch.clamp(self.std(a), -20, 2)\n",
    "        return mu, log_std\n",
    "\n",
    "    def sample(self, obs, test=False):\n",
    "        mu, log_std = self(obs)\n",
    "        if test:\n",
    "            return torch.tanh(mu), 0\n",
    "        n = Normal(mu, log_std.exp())\n",
    "        action = n.rsample().tanh()\n",
    "        log_prob = (n.log_prob(action) - torch.log(1 - action.pow(2) + 1e-6)).sum(1)\n",
    "        return action, log_prob\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, env=None):\n",
    "        super().__init__()\n",
    "        self.obs_dim = env.observation_space.shape[0]             \n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "        input_dim = self.obs_dim + self.action_dim\n",
    "\n",
    "        self.l1 = nn.Linear(input_dim , 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, 1)\n",
    "        \n",
    "        self.apply(weights_init_)\n",
    "\n",
    "    def forward(self, s, act):   \n",
    "        a = torch.cat((s, act), dim=1)\n",
    "        a = F.relu(self.l1(a))\n",
    "        a = F.relu(self.l2(a))\n",
    "        a = self.l3(a)\n",
    "        return a.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"LunarLander-v2\", continuous=True, render_mode=\"rgb_array\")\n",
    "env = gym.make('LunarLanderContinuous-v2', render_mode=\"rgb_array\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "actor = Actor(env).to(device)\n",
    "Q1 = Critic(env).to(device)\n",
    "Q2 = Critic(env).to(device)\n",
    "\n",
    "Q1_ = Critic(env).to(device)\n",
    "Q2_ = Critic(env).to(device)\n",
    "\n",
    "Q1_.load_state_dict(Q1.state_dict())\n",
    "Q2_.load_state_dict(Q2.state_dict())\n",
    "\n",
    "optim_a = torch.optim.Adam(actor.parameters(), lr=0.0001)\n",
    "optim_q1 = torch.optim.Adam(Q1.parameters(), lr=0.0001)\n",
    "optim_q2 = torch.optim.Adam(Q2.parameters(), lr=0.0001)\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "temp = 0.0003     # Temperature of the entropy term\n",
    "train_ep_freq = 1\n",
    "num_opt_steps = 1\n",
    "eval_freq = 20\n",
    "num_eval_ep = 10\n",
    "start_steps = 50000\n",
    "clip_value = 0.5\n",
    "\n",
    "\n",
    "# Loggers\n",
    "rews_log = []\n",
    "rews_eval_log = []\n",
    "steps_log = []\n",
    "loss_q1_log = []\n",
    "loss_q2_log = []\n",
    "loss_a_log = []\n",
    "global_step = 0\n",
    "\n",
    "rb = ReplayBuffer(state_dim=env.observation_space.shape[0], \n",
    "                  action_dim=env.action_space.shape[0], \n",
    "                  length=1000000,\n",
    "                  batch_size=256,\n",
    "                  device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(target, source, tau=0.01):\n",
    "    for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(tau * source_param.data + \n",
    "                                (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(test=False, render=False, random=False):\n",
    "    global global_step\n",
    "    obs, _ = env.reset()\n",
    "    obs = (obs - env.observation_space.low) / (env.observation_space.high - env.observation_space.low) * 2 -1\n",
    "    done = False\n",
    "    rews_ep = []\n",
    "\n",
    "    step = 0\n",
    "    \n",
    "    # Play the game and collect data\n",
    "    while not done:\n",
    "        if random:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = actor.sample(torch.tensor(obs).unsqueeze(0).to(device), test=test)[0]\n",
    "            action = action[0].to('cpu').detach().numpy()\n",
    "    \n",
    "        obs_, reward, terminated, truncated, _ = env.step(action)\n",
    "        obs_ = (obs_ - env.observation_space.low) / (env.observation_space.high - env.observation_space.low) * 2 -1\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        if not truncated:\n",
    "            rb.store(obs, action, reward, obs_, terminated)\n",
    "        obs = obs_\n",
    "        \n",
    "        rews_ep.append(reward)\n",
    "        step += 1\n",
    "        global_step += 1\n",
    "        \n",
    "        # For rendering inside ipynb\n",
    "        if render and step % 5 == 0:\n",
    "                img = env.render()\n",
    "                plt.imshow(img)\n",
    "                plt.axis('off')  # Turn off axis\n",
    "                display(plt.gcf())  # Display the current figure\n",
    "                clear_output(wait=True)  # Clear the previous output\n",
    "    return rews_ep, step\n",
    "\n",
    "def optimize(episode):\n",
    "    s, a, r, s_, d = rb.sample()\n",
    "    \n",
    "    # Optimize the critics\n",
    "    a_theta, log_prob = actor.sample(s_)\n",
    "    q = torch.min(Q1_(s_, a_theta), Q2_(s_, a_theta)).to(device) \n",
    "    \n",
    "    target = r + gamma * (1 - d) * (q - temp * log_prob)\n",
    "    \n",
    "    loss_q1 = F.mse_loss(Q1(s, a), target.detach())\n",
    "    loss_q2 = F.mse_loss(Q2(s, a), target.detach())\n",
    "\n",
    "    Q1.zero_grad()      \n",
    "    loss_q1.backward()     \n",
    "    clip_grad_norm_(Q1.parameters(), clip_value)\n",
    "    optim_q1.step()        \n",
    "    \n",
    "    Q2.zero_grad()\n",
    "    loss_q2.backward()\n",
    "    clip_grad_norm_(Q2.parameters(), clip_value)\n",
    "    optim_q2.step()\n",
    "    \n",
    "    # Optimize the actor\n",
    "    a_theta, log_prob = actor.sample(s)\n",
    "    q = torch.min(Q1(s, a_theta), Q2(s, a_theta)).to(device)\n",
    "    \n",
    "    loss_a = -q.mean() - temp * log_prob.mean()\n",
    "    \n",
    "    actor.zero_grad()\n",
    "    loss_a.backward()\n",
    "    # break\n",
    "    clip_grad_norm_(actor.parameters(), clip_value)\n",
    "    optim_a.step()\n",
    "\n",
    "    loss_q1_log.append(loss_q1.cpu().detach())\n",
    "    loss_q2_log.append(loss_q2.cpu().detach())\n",
    "    loss_a_log.append(loss_a.cpu().detach())\n",
    "    \n",
    "    writer.add_scalar(\"Loss/q1\", loss_q1.cpu().detach(), episode)\n",
    "    writer.add_scalar(\"Loss/q2\", loss_q2.cpu().detach(), episode)\n",
    "    writer.add_scalar(\"Loss/a\", loss_a.cpu().detach(), episode)\n",
    "    writer.add_scalar(\"Loss/entropy\", -temp * log_prob.mean(), episode)\n",
    "    \n",
    "    soft_update(Q1_, Q1, tau=0.01)\n",
    "    soft_update(Q2_, Q2, tau=0.01)\n",
    "\n",
    "# Collect some data before training\n",
    "while global_step < start_steps:\n",
    "    rews_ep, step = run_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/4000000 [00:42<2803:19:55,  2.52s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/meraj/workspace/rl_algorithms/sac/sac.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/meraj/workspace/rl_algorithms/sac/sac.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m episode \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m4000000\u001b[39m)):   \n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/meraj/workspace/rl_algorithms/sac/sac.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     rews_ep, step \u001b[39m=\u001b[39m run_episode()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/meraj/workspace/rl_algorithms/sac/sac.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     rews_log\u001b[39m.\u001b[39mappend(\u001b[39msum\u001b[39m(rews_ep))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/meraj/workspace/rl_algorithms/sac/sac.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     steps_log\u001b[39m.\u001b[39mappend(step)\n",
      "\u001b[1;32m/home/meraj/workspace/rl_algorithms/sac/sac.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/meraj/workspace/rl_algorithms/sac/sac.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     action \u001b[39m=\u001b[39m actor\u001b[39m.\u001b[39msample(torch\u001b[39m.\u001b[39mtensor(obs)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device), test\u001b[39m=\u001b[39mtest)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/meraj/workspace/rl_algorithms/sac/sac.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     action \u001b[39m=\u001b[39m action[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/meraj/workspace/rl_algorithms/sac/sac.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m obs_, reward, terminated, truncated, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/meraj/workspace/rl_algorithms/sac/sac.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m obs_ \u001b[39m=\u001b[39m (obs_ \u001b[39m-\u001b[39m env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mlow) \u001b[39m/\u001b[39m (env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mhigh \u001b[39m-\u001b[39m env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mlow) \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/meraj/workspace/rl_algorithms/sac/sac.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m done \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/envs/box2d/lunar_lander.py:631\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    617\u001b[0m         p\u001b[39m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    618\u001b[0m             (\n\u001b[1;32m    619\u001b[0m                 ox \u001b[39m*\u001b[39m SIDE_ENGINE_POWER \u001b[39m*\u001b[39m s_power,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    624\u001b[0m         )\n\u001b[1;32m    625\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    626\u001b[0m         (\u001b[39m-\u001b[39mox \u001b[39m*\u001b[39m SIDE_ENGINE_POWER \u001b[39m*\u001b[39m s_power, \u001b[39m-\u001b[39moy \u001b[39m*\u001b[39m SIDE_ENGINE_POWER \u001b[39m*\u001b[39m s_power),\n\u001b[1;32m    627\u001b[0m         impulse_pos,\n\u001b[1;32m    628\u001b[0m         \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    629\u001b[0m     )\n\u001b[0;32m--> 631\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mworld\u001b[39m.\u001b[39;49mStep(\u001b[39m1.0\u001b[39;49m \u001b[39m/\u001b[39;49m FPS, \u001b[39m6\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m30\u001b[39;49m, \u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m30\u001b[39;49m)\n\u001b[1;32m    633\u001b[0m pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mposition\n\u001b[1;32m    634\u001b[0m vel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mlinearVelocity\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/envs/box2d/lunar_lander.py:75\u001b[0m, in \u001b[0;36mContactDetector.EndContact\u001b[0;34m(self, contact)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mlegs[i] \u001b[39min\u001b[39;00m [contact\u001b[39m.\u001b[39mfixtureA\u001b[39m.\u001b[39mbody, contact\u001b[39m.\u001b[39mfixtureB\u001b[39m.\u001b[39mbody]:\n\u001b[1;32m     73\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mlegs[i]\u001b[39m.\u001b[39mground_contact \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mEndContact\u001b[39m(\u001b[39mself\u001b[39m, contact):\n\u001b[1;32m     76\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m):\n\u001b[1;32m     77\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mlegs[i] \u001b[39min\u001b[39;00m [contact\u001b[39m.\u001b[39mfixtureA\u001b[39m.\u001b[39mbody, contact\u001b[39m.\u001b[39mfixtureB\u001b[39m.\u001b[39mbody]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in tqdm(range(4000000)):   \n",
    "    rews_ep, step = run_episode()\n",
    "    \n",
    "    rews_log.append(sum(rews_ep))\n",
    "    steps_log.append(step)\n",
    "\n",
    "    if episode % eval_freq == 0:\n",
    "        eval_rews = []\n",
    "        eval_steps = []\n",
    "        for _ in range(num_eval_ep):\n",
    "            rews_ep, step = run_episode(test=True)\n",
    "            eval_rews.append(sum(rews_ep))\n",
    "            eval_steps.append(step)\n",
    "        rews_eval_log.append(sum(eval_rews) / len(eval_rews))\n",
    "        writer.add_scalar(\"Eval/reward\", sum(eval_rews) / len(eval_rews), episode)\n",
    "        writer.add_scalar(\"Eval/step\", sum(eval_steps) / len(eval_steps), episode)\n",
    "\n",
    "    if episode % train_ep_freq == 0:\n",
    "        for opt_step in range(num_opt_steps):\n",
    "            optimize(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,3,1)\n",
    "plt.plot(rews_eval_log)\n",
    "plt.grid()\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(rews_log)\n",
    "plt.grid()\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(steps_log)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(loss_q1_log[x:])\n",
    "plt.grid()\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(loss_q2_log[x:])\n",
    "plt.grid()\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(loss_a_log[x:])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_rews, steps = run_episode(test=True, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(ep_rews)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
