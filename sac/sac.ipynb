{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Soft Actor Critic (SAC) implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "from model import ReplayBuffer, SquashedNormal\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env=None):\n",
    "        super().__init__()\n",
    "        self.input_dim = env.observation_space.shape[0]             \n",
    "        self.output_dim = env.action_space.shape[0]\n",
    "        \n",
    "        self.l1 = nn.Linear(self.input_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        \n",
    "        self.mu = nn.Linear(300, self.output_dim)\n",
    "        self.std = nn.Linear(300, self.output_dim)\n",
    "        \n",
    "        self.apply(weights_init_)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = F.relu(self.l1(x))\n",
    "        a = F.relu(self.l2(a))\n",
    "        \n",
    "        mu = self.mu(a)\n",
    "        log_std = torch.clamp(self.std(a), -20, 2)        \n",
    "        return mu, log_std\n",
    "\n",
    "    def sample(self, obs, test=False):\n",
    "        mu, log_std = self(obs)\n",
    "        if test:\n",
    "            return torch.tanh(mu), 0\n",
    "        # n = Normal(mu, log_std.exp())\n",
    "        n = SquashedNormal(mu, log_std.exp())\n",
    "        action = n.rsample().tanh()\n",
    "        log_prob = (n.log_prob(action) - torch.log(1 - action.pow(2) + 1e-6)).sum(1)\n",
    "        return action, log_prob\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, env=None):\n",
    "        super().__init__()\n",
    "        self.obs_dim = env.observation_space.shape[0]             \n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "        input_dim = self.obs_dim + self.action_dim\n",
    "\n",
    "        self.l1 = nn.Linear(input_dim , 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, 1)\n",
    "        \n",
    "        self.apply(weights_init_)\n",
    "\n",
    "    def forward(self, s, act):   \n",
    "        a = torch.cat((s, act), dim=1)\n",
    "        a = F.relu(self.l1(a))\n",
    "        a = F.relu(self.l2(a))\n",
    "        a = self.l3(a)\n",
    "        return a.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"LunarLander-v2\", continuous=True, render_mode=\"rgb_array\")\n",
    "env = gym.make('LunarLanderContinuous-v2', render_mode=\"rgb_array\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "actor = Actor(env).to(device)\n",
    "Q1 = Critic(env).to(device)\n",
    "Q2 = Critic(env).to(device)\n",
    "\n",
    "Q1_ = Critic(env).to(device)\n",
    "Q2_ = Critic(env).to(device)\n",
    "\n",
    "Q1_.load_state_dict(Q1.state_dict())\n",
    "Q2_.load_state_dict(Q2.state_dict())\n",
    "\n",
    "optim_a = torch.optim.Adam(actor.parameters(), lr=0.0001)\n",
    "optim_q1 = torch.optim.Adam(Q1.parameters(), lr=0.0001)\n",
    "optim_q2 = torch.optim.Adam(Q2.parameters(), lr=0.0001)\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "temp = 0.0003     # Temperature of the entropy term\n",
    "train_ep_freq = 1\n",
    "num_opt_steps = 1\n",
    "eval_freq = 20\n",
    "num_eval_ep = 10\n",
    "start_steps = 50000\n",
    "clip_value = 0.5\n",
    "\n",
    "\n",
    "# Loggers\n",
    "rews_log = []\n",
    "rews_eval_log = []\n",
    "steps_log = []\n",
    "loss_q1_log = []\n",
    "loss_q2_log = []\n",
    "loss_a_log = []\n",
    "global_step = 0\n",
    "\n",
    "rb = ReplayBuffer(state_dim=env.observation_space.shape[0], \n",
    "                  action_dim=env.action_space.shape[0], \n",
    "                  length=1000000,\n",
    "                  batch_size=256,\n",
    "                  device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(target, source, tau=0.01):\n",
    "    for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(tau * source_param.data + \n",
    "                                (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(test=False, render=False, random=False):\n",
    "    global global_step\n",
    "    obs, _ = env.reset()\n",
    "    obs = (obs - env.observation_space.low) / (env.observation_space.high - env.observation_space.low) * 2 -1\n",
    "    done = False\n",
    "    rews_ep = []\n",
    "\n",
    "    step = 0\n",
    "    \n",
    "    # Play the game and collect data\n",
    "    while not done:\n",
    "        if random:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = actor.sample(torch.tensor(obs).unsqueeze(0).to(device), test=test)[0]\n",
    "            action = action[0].to('cpu').detach().numpy()\n",
    "    \n",
    "        obs_, reward, terminated, truncated, _ = env.step(action)\n",
    "        obs_ = (obs_ - env.observation_space.low) / (env.observation_space.high - env.observation_space.low) * 2 -1\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        if not truncated:\n",
    "            rb.store(obs, action, reward, obs_, terminated)\n",
    "        obs = obs_\n",
    "        \n",
    "        rews_ep.append(reward)\n",
    "        step += 1\n",
    "        global_step += 1\n",
    "        \n",
    "        # For rendering inside ipynb\n",
    "        if render and step % 5 == 0:\n",
    "                img = env.render()\n",
    "                plt.imshow(img)\n",
    "                plt.axis('off')  # Turn off axis\n",
    "                display(plt.gcf())  # Display the current figure\n",
    "                clear_output(wait=True)  # Clear the previous output\n",
    "    \n",
    "        if not test:\n",
    "            optimize(episode)\n",
    "    return rews_ep, step\n",
    "\n",
    "def optimize(episode):\n",
    "    s, a, r, s_, d = rb.sample()\n",
    "    \n",
    "    # Optimize the critics\n",
    "    a_theta, log_prob = actor.sample(s_)\n",
    "    q = torch.min(Q1_(s_, a_theta), Q2_(s_, a_theta)).to(device) \n",
    "    \n",
    "    target = r + gamma * (1 - d) * (q - temp * log_prob)\n",
    "    \n",
    "    loss_q1 = F.mse_loss(Q1(s, a), target.detach())\n",
    "    loss_q2 = F.mse_loss(Q2(s, a), target.detach())\n",
    "\n",
    "    Q1.zero_grad()      \n",
    "    loss_q1.backward()     \n",
    "    clip_grad_norm_(Q1.parameters(), clip_value)\n",
    "    optim_q1.step()        \n",
    "    \n",
    "    Q2.zero_grad()\n",
    "    loss_q2.backward()\n",
    "    clip_grad_norm_(Q2.parameters(), clip_value)\n",
    "    optim_q2.step()\n",
    "    \n",
    "    # Optimize the actor\n",
    "    a_theta, log_prob = actor.sample(s)\n",
    "    q = torch.min(Q1(s, a_theta), Q2(s, a_theta)).to(device)\n",
    "    \n",
    "    loss_a = -q.mean() - temp * log_prob.mean()\n",
    "    \n",
    "    actor.zero_grad()\n",
    "    loss_a.backward()\n",
    "    # break\n",
    "    clip_grad_norm_(actor.parameters(), clip_value)\n",
    "    optim_a.step()\n",
    "\n",
    "    loss_q1_log.append(loss_q1.cpu().detach())\n",
    "    loss_q2_log.append(loss_q2.cpu().detach())\n",
    "    loss_a_log.append(loss_a.cpu().detach())\n",
    "    \n",
    "    writer.add_scalar(\"Loss/q1\", loss_q1.cpu().detach(), episode)\n",
    "    writer.add_scalar(\"Loss/q2\", loss_q2.cpu().detach(), episode)\n",
    "    writer.add_scalar(\"Loss/a\", loss_a.cpu().detach(), episode)\n",
    "    writer.add_scalar(\"Loss/entropy\", -temp * log_prob.mean(), episode)\n",
    "    \n",
    "    soft_update(Q1_, Q1, tau=0.001)\n",
    "    soft_update(Q2_, Q2, tau=0.001)\n",
    "\n",
    "# Collect some data before training\n",
    "while global_step < start_steps:\n",
    "    rews_ep, step = run_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in tqdm(range(4000000)):   \n",
    "    rews_ep, step = run_episode()\n",
    "    \n",
    "    rews_log.append(sum(rews_ep))\n",
    "    steps_log.append(step)\n",
    "\n",
    "    if episode % eval_freq == 0:\n",
    "        eval_rews = []\n",
    "        eval_steps = []\n",
    "        for _ in range(num_eval_ep):\n",
    "            rews_ep, step = run_episode(test=True)\n",
    "            eval_rews.append(sum(rews_ep))\n",
    "            eval_steps.append(step)\n",
    "        rews_eval_log.append(sum(eval_rews) / len(eval_rews))\n",
    "        writer.add_scalar(\"Eval/reward\", sum(eval_rews) / len(eval_rews), episode)\n",
    "        writer.add_scalar(\"Eval/step\", sum(eval_steps) / len(eval_steps), episode)\n",
    "\n",
    "    # if episode % train_ep_freq == 0:\n",
    "    #     for opt_step in range(num_opt_steps):\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,3,1)\n",
    "plt.plot(rews_eval_log)\n",
    "plt.grid()\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(rews_log)\n",
    "plt.grid()\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(steps_log)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(loss_q1_log[x:])\n",
    "plt.grid()\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(loss_q2_log[x:])\n",
    "plt.grid()\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(loss_a_log[x:])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_rews, steps = run_episode(test=True, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(ep_rews)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
